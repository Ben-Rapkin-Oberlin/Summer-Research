{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"DKhGjRvvO8ek","execution":{"iopub.status.busy":"2023-06-02T20:27:46.350221Z","iopub.execute_input":"2023-06-02T20:27:46.350621Z","iopub.status.idle":"2023-06-02T20:27:46.357493Z","shell.execute_reply.started":"2023-06-02T20:27:46.350577Z","shell.execute_reply":"2023-06-02T20:27:46.356278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#!pip install ray\n#!pip install -U tensorboardx\n#!pip install pyserial\n\nimport numpy as np \nimport pandas as pd \n\nimport os\n#import serial #import write\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.datasets import FashionMNIST\nfrom torchmetrics import Accuracy\n\nfrom ray import air, tune\nfrom ray.air import Checkpoint, session\nfrom ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n\n","metadata":{"id":"hYBzEiWMO8em","outputId":"e943edeb-12df-4b7f-e69b-4360e5cfe1ec","execution":{"iopub.status.busy":"2023-06-02T20:27:46.361982Z","iopub.execute_input":"2023-06-02T20:27:46.362507Z","iopub.status.idle":"2023-06-02T20:27:46.372982Z","shell.execute_reply.started":"2023-06-02T20:27:46.362476Z","shell.execute_reply":"2023-06-02T20:27:46.371482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title old net\nclass Net(nn.Module):\n    def __init__(self, l1=12, l2=10,p=0.2,act=\"ReLU\",batch=False):\n        super(Net, self).__init__()\n        \"\"\"\n        self.b0=nn.BatchNorm1d(784)\n        self.fc1=nn.Linear(784, l1) # 16 input features, 12 output features also called neurons\n        self.b1=nn.BatchNorm1d(l1)\n        self.fc2=nn.Linear(l1, l2)\n        self.b2=nn.BatchNorm1d(l2)\n        self.out=nn.Linear(l2, 10)\n        self.m = nn.LogSoftmax(dim=0)\n        \"\"\"\n        drop = nn.Dropout(p)\n        activation_functions={\n            \"ReLU\": nn.ReLU(),\n            \"PReLU\": nn.PReLU(),\n            \"LeakyReLU\":nn.LeakyReLU(),\n            \"SELU\":nn.SELU(),\n            \"RReLU\":nn.RReLU(),\n            \"ELU\":nn.ELU(),\n            \"SiLU\":nn.SiLU(),\n            \"Sigmoid\":nn.Sigmoid(),\n            \"Mish\":nn.Mish(),\n        }\n        func=activation_functions[act]\n        #print(self.act)\n        \n        self.layer1=nn.Sequential(nn.BatchNorm1d(784),\n                                nn.Linear(784, l1),\n                                func,\n                                nn.BatchNorm1d(l1),\n                                drop,\n                               )\n        self.layer2=nn.Sequential(nn.Linear(l1,l2),\n                                func,\n                                nn.BatchNorm1d(l2),\n                                drop,\n                               )\n        \n        self.layer3=nn.Sequential(nn.Linear(l2, 10),\n                                nn.LogSoftmax(dim=0),\n                               )\n            \n    def forward(self, x):\n        \"\"\"\n        x = x.squeeze()\n        x = self.fc1(x)\n        #x = F.relu(x)\n        x = self.act(x)\n        x = self.drop(x)\n       \n        x = self.fc2(x)\n        x = self.act(x)\n        x = self.drop(x)\n       \n        x = self.out(x)\n        output = self.m(x)\n        \"\"\"\n               \n        x = x.squeeze()\n        x = self.layer1(x)\n        x = self.layer2(x)\n        output = self.layer3(x)\n        \n        return output\n        ","metadata":{"cellView":"form","id":"owQqUbwSO8eo","execution":{"iopub.status.busy":"2023-06-02T20:27:46.374807Z","iopub.execute_input":"2023-06-02T20:27:46.375396Z","iopub.status.idle":"2023-06-02T20:27:46.388231Z","shell.execute_reply.started":"2023-06-02T20:27:46.375364Z","shell.execute_reply":"2023-06-02T20:27:46.387041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title og load data\ndef load_data(data_dir=\"/kaggle/working/\"):\n    training_data = torchvision.datasets.FashionMNIST(\n    root=\"data_dir\",\n    train=True,\n    download=True,\n    transform=transforms.ToTensor()\n    )\n\n    test_data = torchvision.datasets.FashionMNIST(\n    root=\"data_dir\",\n    train=False,\n    download=True,\n    transform=transforms.ToTensor()\n    )\n    training_data.data=torch.flatten(training_data.data, start_dim=1)\n    test_data.data=torch.flatten(test_data.data, start_dim=1)\n    #print(training_data.data.shape)\n    return training_data, test_data\n#t,te=load_data()\n#print(t.data.shape)\n#print(te.data.shape)","metadata":{"cellView":"form","id":"OlsNDlqWO8eq","outputId":"cf24278e-69e8-45a8-9831-f81603988306","execution":{"iopub.status.busy":"2023-06-02T20:27:46.390441Z","iopub.execute_input":"2023-06-02T20:27:46.390828Z","iopub.status.idle":"2023-06-02T20:27:46.403992Z","shell.execute_reply.started":"2023-06-02T20:27:46.390793Z","shell.execute_reply":"2023-06-02T20:27:46.402736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title accuracy\ndef test_accuracy(net, device=\"cpu\"):\n    _, testset = load_data()\n\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=4, shuffle=False, num_workers=2\n    )\n\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        c=0\n        for data in testloader:\n                  #  c+=1\n                    #if (c==2500):\n                       # print('g',c)\n                    #    break\n                    images, labels = data\n                    images, labels = images.to(device), labels.to(device)\n                    #print(images.shape)\n                    outputs = net(images)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n    return correct / total\n#print(test_accuracy(Net(16,8)))\n#print(test_accuracy(Net(16,8)))\n#print(test_accuracy(Net(16,8)))","metadata":{"id":"1WTMJ6B1O8er","execution":{"iopub.status.busy":"2023-06-02T20:27:46.405624Z","iopub.execute_input":"2023-06-02T20:27:46.406552Z","iopub.status.idle":"2023-06-02T20:27:46.418996Z","shell.execute_reply.started":"2023-06-02T20:27:46.406506Z","shell.execute_reply":"2023-06-02T20:27:46.418008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_cifar(config,epochs=10,data_dir=None,trainset=None,testset=None,tune=False,dataloaders=None,max_iter=10):\n    net = Net(l1=config[\"l1\"],l2=config[\"l2\"],p=config[\"dropout\"],act=config[\"act\"])\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if torch.cuda.device_count() > 1:\n            net = nn.DataParallel(net)\n            \n    net.to(device)\n    criterion = nn.CrossEntropyLoss()\n\n    if config[\"opt\"]==\"SGD\":\n        optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"])#, momentum=.9)\n        \n    elif config[\"opt\"]==\"Adam\":\n        optimizer = optim.Adam(net.parameters(),lr=config[\"lr\"])\n        \n    elif config[\"opt\"]==\"LBFGS\":\n        optimizer = optim.LBFGS(net.parameters(),lr=config[\"lr\"])\n        \n    elif config[\"opt\"]==\"NAdam\":\n        optimizer = optim.NAdam(net.parameters(),lr=config[\"lr\"])\n        \n        \n    start_epoch = 0\n    if tune:\n        checkpoint = session.get_checkpoint()\n\n        if checkpoint:\n            checkpoint_state = checkpoint.to_dict()\n            start_epoch = checkpoint_state[\"epoch\"]\n            net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n\n    if dataloaders==None:\n        test_abs = int(len(trainset) * 0.8)\n        train_subset, val_subset = random_split(\n            trainset, [test_abs, len(trainset) - test_abs]\n        )\n\n        trainloader = torch.utils.data.DataLoader(\n            train_subset, batch_size=32, shuffle=True, num_workers=4\n        )\n        valloader = torch.utils.data.DataLoader(\n            val_subset, batch_size=32, shuffle=True, num_workers=4\n        )\n    else:\n        trainloader=dataloaders[0]\n        valloader=dataloaders[1]\n    \n    \n    IterMet={\"acc_\"+str(i): -1 for i in range(1,max_iter+1)} | {\"loss_\"+str(i): -1 for i in range(1,max_iter+1)} \n    IterMet['loss']=-1   \n    IterMet['accuracy']=-1    \n    \n    for epoch in range(start_epoch, epochs):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            outputs = net(inputs)\n                        \n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_steps += 1\n           \n\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        total = 0\n        correct = 0\n        for i, data in enumerate(valloader, 0):\n            with torch.no_grad():\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = net(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.cpu().numpy()\n                val_steps += 1\n        \n        if tune:\n            checkpoint_data = {\n                       \"epoch\": epoch,\n                        \"net_state_dict\": net.state_dict(),\n                        \"optimizer_state_dict\": optimizer.state_dict(),\n            }\n            \n            checkpoint = Checkpoint.from_dict(checkpoint_data)\n            acc=correct / total\n            loss=val_loss / val_steps\n            \n            IterMet[\"acc_\"+str(epoch+1)]=acc\n            IterMet[\"loss_\"+str(epoch+1)]=loss\n            IterMet[\"accuracy\"]=acc\n            IterMet[\"loss\"]=loss\n            \n            session.report(\n                        IterMet,\n                        checkpoint=checkpoint,\n                )\n\n        else:\n            if epoch%5==0:\n                print(\"epoch,val loss,%correct\",epoch,val_loss,correct/total)\n    print(\"Finished Training\")\n    \n    return net","metadata":{"id":"9judIZFoO8es","execution":{"iopub.status.busy":"2023-06-02T20:27:46.538188Z","iopub.execute_input":"2023-06-02T20:27:46.539370Z","iopub.status.idle":"2023-06-02T20:27:46.560994Z","shell.execute_reply.started":"2023-06-02T20:27:46.539316Z","shell.execute_reply":"2023-06-02T20:27:46.560135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title hp3\ndef hyperparam_search2(config,scheduler=\"ASHA\",cpu=4,gpu=0,max_num_epochs=10,concurrent=1):\n    if scheduler==\"ASHA\":\n        sched = ASHAScheduler(\n            #metric=\"loss\",\n            #mode=\"min\",\n            #mode=\"max\",\n            max_t=max_num_epochs,\n            grace_period=1,\n            reduction_factor=2,\n            brackets=1,\n        )\n    \n    if scheduler==\"PB2\":\n        pass\n    \n    trainset, testset = load_data() \n    test_abs = int(len(trainset) * 0.8)\n    train_subset, val_subset = random_split(\n            trainset, [test_abs, len(trainset) - test_abs]\n    )\n   \n    trainloader = torch.utils.data.DataLoader(\n            train_subset, batch_size=64, shuffle=True, num_workers=4\n    )\n    valloader = torch.utils.data.DataLoader(\n            val_subset, batch_size=64, shuffle=True, num_workers=4\n    )\n\n    tuner = tune.Tuner(\n        #tune.with_parameters(train_cifar,trainset=trainset,testset=testset,tune=True),\n        tune.with_parameters(train_cifar,dataloaders=[trainloader,valloader],tune=True,max_iter=max_num_epochs),\n        param_space=config,\n        tune_config=tune.TuneConfig(\n           # num_samples=num_samples, different when using grid search\n            metric=\"accuracy\",\n            mode=\"max\",\n            scheduler=sched,\n            max_concurrent_trials=concurrent\n            )\n    )\n       \n    results = tuner.fit()\n    #session.shutdown()\n    best_trial = results.get_best_result(metric=\"accuracy\", mode=\"max\")\n   # all = results.get_results().\n    df=results.get_dataframe()\n    dfconfig=pd.DataFrame(best_trial.config,index=[0])\n    dfmetrics=pd.DataFrame(best_trial.metrics,index=[0])\n    \n    df.to_csv(\"/kaggle/working/NAdam_search_results.csv\")\n    dfconfig.to_csv(\"/kaggle/working/NAdam_best_config.csv\")\n    dfmetrics.to_csv(\"/kaggle/working/NAdam_best_metrics.csv\")\n\n    print(type(best_trial.config))\n    print(f\"Best trial config: {best_trial.config}\")\n    print(f\"Best trial final validation loss: {best_trial.metrics['loss']}\")\n    print(f\"Best trial final validation accuracy: {best_trial.metrics['accuracy']}\")\n    \n    return best_trial.config\n    ","metadata":{"id":"rBUie-Du_Rpq","execution":{"iopub.status.busy":"2023-06-02T20:27:46.563235Z","iopub.execute_input":"2023-06-02T20:27:46.563989Z","iopub.status.idle":"2023-06-02T20:27:46.579635Z","shell.execute_reply.started":"2023-06-02T20:27:46.563956Z","shell.execute_reply":"2023-06-02T20:27:46.578303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title og hps\ndef hyperparam_search(config,scheduler=\"ASHA\",cpu=4,gpu=0,num_samples=20,max_num_epochs=10):\n    \n    #determins how the searching works\n    if scheduler==\"ASHA\":\n        #tries various permutations of parameters\n        scheduler = ASHAScheduler(\n            metric=\"loss\",\n            #mode=\"min\",\n            mode=\"max\",\n            max_t=max_num_epochs,\n            grace_period=1,\n            reduction_factor=2,\n        )\n    \n    if scheduler==\"PB2\":\n        pass\n    \n    \n    trainset, testset = load_data()\n    \n    \"\"\"\n    result = tune.run(\n        partial(train_cifar,trainset=trainset,testset=testset,tune=True),\n        resources_per_trial={\"cpu\": cpu, \"gpu\": gpu},\n        config=config,\n        #num_samples=num_samples,\n        scheduler=scheduler,\n        verbose=1\n        from functools import partial\n        \n    )\n    \"\"\"\n    return    \n    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n    \n    print(f\"Best trial config: {best_trial.config}\")\n    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n    \n\n    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if gpus_per_trial > 1:\n            best_trained_model = nn.DataParallel(best_trained_model)\n    best_trained_model.to(device)\n\n    best_checkpoint = best_trial.checkpoint.to_air_checkpoint()\n    best_checkpoint_data = best_checkpoint.to_dict()\n    best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n    test_acc = test_accuracy(best_trained_model, device)\n    print(\"Best trial test set accuracy: {}\".format(test_acc))\n    return best_trial.config\n    ","metadata":{"cellView":"form","id":"KMMaWRxzO8ev","execution":{"iopub.status.busy":"2023-06-02T20:27:46.580985Z","iopub.execute_input":"2023-06-02T20:27:46.581314Z","iopub.status.idle":"2023-06-02T20:27:46.597608Z","shell.execute_reply.started":"2023-06-02T20:27:46.581288Z","shell.execute_reply":"2023-06-02T20:27:46.596686Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    torch.manual_seed(0)  \n    search_space = {\n        #\"opt\": tune.grid_search([\"NAdam\",\"LBFGS\",\"Adam\",\"SGD\"]),\n        \"opt\": \"NAdam\",\n        \"l1\": tune.grid_search([2**i for i in range(4,8)]),\n        \"l2\": tune.grid_search([2**i for i in range(4,7)]),\n        \"lr\": tune.grid_search([0.1,0.01,0.001]),\n        \"dropout\": tune.grid_search([0.12,.16,0.2,0.24]),\n         #\"dropout\": tune.grid_search([0.15,0.2,0.25]),\n         \"act\": tune.grid_search([\"ReLU\",\n                                  \"PReLU\",\n                                  \"LeakyReLU\",\n                                  \"SELU\",\n                                  \"RReLU\",                               \n                                  \"ELU\",\n                                  \"SiLU\",\n                                  \"Sigmoid\",\n                                  \"Mish\",\n                                 ]\n                                )\n    }\n                                  \n    #SNN \n    #batch, layer, group\n                                 \n                                 \n    quick_space = {\n        \"opt\": \"SGD\",\n        \"l1\": tune.grid_search([2**i for i in range(4,7)]),\n        #\"l1\": 20,\n        \"l2\":16,\n        \"lr\": .001,\n        #\"dropout\": np.random.uniform(low=.1, high=.3),\n        \"dropout\": .2,\n        \"act\": \"ReLU\"\n    }\n    \n    \n\n    #train_cifar(dataloaders=[trainloader,valloader],config=quick_space)\n    \n    #print(s)\n    \n    \n    #best_config=hyperparam_search2(quick_space,scheduler=\"ASHA\",max_num_epochs=3,concurrent=0)\n    best_config=hyperparam_search2(search_space,scheduler=\"ASHA\",max_num_epochs=10,concurrent=0)\n    #serial.write(b'\\x03')\n    \n    \"\"\"\n    print(\"--------training model--------\")\n    trainset, testset = load_data() \n    test_abs = int(len(trainset) * 0.8)\n    train_subset, val_subset = random_split(\n            trainset, [test_abs, len(trainset) - test_abs]\n    )\n   \n    trainloader = torch.utils.data.DataLoader(\n            train_subset, batch_size=64, shuffle=True, num_workers=4\n    )\n    valloader = torch.utils.data.DataLoader(\n            val_subset, batch_size=64, shuffle=True, num_workers=4\n    )\n\n    net=train_cifar(best_config,epochs=200,dataloaders=[trainloader,valloader])\n    test_acc = test_accuracy(net)\n    print(\"Best trial test set accuracy: {}\".format(test_acc))\n    torch.save(net,'/working')\n    \"\"\"","metadata":{"id":"vACehOR2O8ew","execution":{"iopub.status.busy":"2023-06-02T20:27:46.599378Z","iopub.execute_input":"2023-06-02T20:27:46.600264Z","iopub.status.idle":"2023-06-02T20:27:46.615047Z","shell.execute_reply.started":"2023-06-02T20:27:46.600230Z","shell.execute_reply":"2023-06-02T20:27:46.614034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"id":"w4hKqCTEO8ew","outputId":"4a247ea5-d8e9-428d-edaa-0a5115e24388","execution":{"iopub.status.busy":"2023-06-02T20:27:46.616168Z","iopub.execute_input":"2023-06-02T20:27:46.618913Z","iopub.status.idle":"2023-06-02T20:29:12.435465Z","shell.execute_reply.started":"2023-06-02T20:27:46.618867Z","shell.execute_reply":"2023-06-02T20:29:12.434196Z"},"trusted":true},"execution_count":null,"outputs":[]}]}